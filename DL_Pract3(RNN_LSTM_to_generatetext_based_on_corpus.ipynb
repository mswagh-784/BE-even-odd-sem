{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 2124,
          "sourceType": "datasetVersion",
          "datasetId": 1028
        }
      ],
      "dockerImageVersionId": 31154,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "DL-Pract3(RNN/LSTM to generatetext based on corpus",
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "kingburrito666_shakespeare_plays_path = kagglehub.dataset_download('kingburrito666/shakespeare-plays')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwJ31aMSDxSH",
        "outputId": "511d7cc8-a0ff-4588-ba00-3a39b37ede48"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/kingburrito666/shakespeare-plays?dataset_version_number=4...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.55M/4.55M [00:00<00:00, 194MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Data source import complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-09T12:04:50.649522Z",
          "iopub.execute_input": "2025-11-09T12:04:50.650342Z",
          "iopub.status.idle": "2025-11-09T12:04:50.664265Z",
          "shell.execute_reply.started": "2025-11-09T12:04:50.650312Z",
          "shell.execute_reply": "2025-11-09T12:04:50.663276Z"
        },
        "id": "lX6L95WQDxSI"
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------\n",
        "# 1. IMPORTS\n",
        "# --------------------------------------------------------------\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 2. LOAD & PRE-PROCESS TEXT (use only first 500k chars)\n",
        "# --------------------------------------------------------------\n",
        "# Use the path returned by kagglehub.dataset_download in the first cell\n",
        "data_dir = kingburrito666_shakespeare_plays_path\n",
        "text = ''\n",
        "for fn in os.listdir(data_dir):\n",
        "    if fn.endswith('.txt'):\n",
        "        with open(os.path.join(data_dir, fn), 'r', encoding='utf-8') as f:\n",
        "            text += f.read() + '\\n'\n",
        "\n",
        "# ---- TAKE SUBSET ------------------------------------------------\n",
        "text = text[:500_000]                     # <<< fast demo\n",
        "print(f\"Using {len(text):,} characters\")\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_ix = {c: i for i, c in enumerate(chars)}\n",
        "ix_to_char = {i: c for i, c in enumerate(chars)}\n",
        "vocab_size = len(chars)\n",
        "print(f\"Vocab size: {vocab_size}\")\n",
        "\n",
        "def text_to_tensor(s):\n",
        "    return torch.tensor([char_to_ix[c] for c in s], dtype=torch.long)\n",
        "\n",
        "data = text_to_tensor(text)\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 3. DATASET (seq_len = 50)\n",
        "# --------------------------------------------------------------\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, data, seq_len=50):\n",
        "        self.data = data\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.seq_len\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.data[i:i+self.seq_len],\n",
        "                self.data[i+1:i+self.seq_len+1])\n",
        "\n",
        "seq_len = 50\n",
        "dataset = CharDataset(data, seq_len)\n",
        "dataloader = DataLoader(dataset, batch_size=128, shuffle=True, drop_last=True)\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 4. LSTM MODEL\n",
        "# --------------------------------------------------------------\n",
        "class CharLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=64, hidden_dim=256, layers=2):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm  = nn.LSTM(embed_dim, hidden_dim, layers,\n",
        "                             dropout=0.3, batch_first=True)\n",
        "        self.fc    = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embed(x)                     # (B, L, E)\n",
        "        out, hidden = self.lstm(x, hidden)    # (B, L, H)\n",
        "        out = out.contiguous().view(-1, out.size(-1))\n",
        "        out = self.fc(out)                    # (B*L, V)\n",
        "        return out, hidden\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = CharLSTM(vocab_size).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 5. TRAINING LOOP (5 epochs, progress bar, grad clipping)\n",
        "# --------------------------------------------------------------\n",
        "def train(epochs=5):\n",
        "    model.train()\n",
        "    for ep in range(1, epochs+1):\n",
        "        epoch_loss = 0.0\n",
        "        pbar = tqdm(dataloader, desc=f'Epoch {ep}/{epochs}')\n",
        "        for xb, yb in pbar:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits, _ = model(xb)\n",
        "            loss = criterion(logits, yb.view(-1))\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        avg = epoch_loss / len(dataloader)\n",
        "        print(f\"\\n>>> Epoch {ep} finished – Avg loss: {avg:.4f}\\n\")\n",
        "\n",
        "train(epochs=5)\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 6. SAVE MODEL\n",
        "# --------------------------------------------------------------\n",
        "torch.save(model.state_dict(), 'shakespeare_lstm_fast.pth')\n",
        "print(\"Model saved → shakespeare_lstm_fast.pth\")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 7. TEXT GENERATION (temperature sampling)\n",
        "# --------------------------------------------------------------\n",
        "def generate(seed: str, length: int = 300, temp: float = 0.8):\n",
        "    model.eval()\n",
        "    generated = list(seed)\n",
        "    # pad seed to seq_len if needed\n",
        "    if len(generated) < seq_len:\n",
        "        generated = [' '] * (seq_len - len(generated)) + generated\n",
        "    inp = torch.tensor([char_to_ix.get(c, 0) for c in generated[-seq_len:]],\n",
        "                       dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    hidden = None\n",
        "    with torch.no_grad():\n",
        "        for _ in range(length):\n",
        "            logits, hidden = model(inp, hidden)\n",
        "            probs = torch.softmax(logits[-1] / temp, dim=-1)\n",
        "            nxt = torch.multinomial(probs, num_samples=1).item()\n",
        "            generated.append(ix_to_char[nxt])\n",
        "            # slide window\n",
        "            inp = torch.cat([inp[:, 1:], torch.tensor([[nxt]], device=device)], dim=1)\n",
        "\n",
        "    return ''.join(generated[-length:])   # return only the newly generated part\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 8. TRY IT\n",
        "# --------------------------------------------------------------\n",
        "seed_text = \"To be, or not to be\"\n",
        "print(\"\\n--- GENERATED TEXT ---\")\n",
        "print(generate(seed_text, length=400, temp=0.8))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-09T12:04:50.665546Z",
          "iopub.execute_input": "2025-11-09T12:04:50.665828Z",
          "iopub.status.idle": "2025-11-09T12:10:23.210271Z",
          "shell.execute_reply.started": "2025-11-09T12:04:50.665802Z",
          "shell.execute_reply": "2025-11-09T12:10:23.209492Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFWI2sZqDxSJ",
        "outputId": "d8369903-ccad-4fd7-f271-db87baf75b9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 500,000 characters\n",
            "Vocab size: 70\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 3905/3905 [01:04<00:00, 60.34it/s, loss=1.28]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> Epoch 1 finished – Avg loss: 1.5354\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 3905/3905 [01:03<00:00, 61.26it/s, loss=1.13]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> Epoch 2 finished – Avg loss: 1.2129\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 3905/3905 [01:03<00:00, 61.31it/s, loss=1.09]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> Epoch 3 finished – Avg loss: 1.1228\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 3905/3905 [01:03<00:00, 61.05it/s, loss=1.07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> Epoch 4 finished – Avg loss: 1.0730\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 3905/3905 [01:04<00:00, 61.00it/s, loss=1.05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> Epoch 5 finished – Avg loss: 1.0401\n",
            "\n",
            "Model saved → shakespeare_lstm_fast.pth\n",
            "\n",
            "--- GENERATED TEXT ---\n",
            "hold\"\n",
            "\"which three and digg'd you had he break in their house.\"\n",
            "\"An if thou particular unto his soul.\"\n",
            "\"And God forbid a silly steel at once,\"\n",
            "\"The Lord Scot graceously in France,\"\n",
            "\"And now no ear by was the truth, and tell the king.\"\n",
            "\"Therefore, to brave herreat: possession and thy\"\n",
            "\"brother of commonwealth,\"\n",
            "\"How shall be commanded with a scorron sweet, but instandared,\"\n",
            "\"That we to take my lord\n"
          ]
        }
      ],
      "execution_count": 5
    }
  ]
}